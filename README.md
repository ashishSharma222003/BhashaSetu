# BhashaSetu - Hindi Language Conversational AI Pipeline

![Project Structure](https://img.shields.io/badge/Project%20Structure-Modular-brightgreen)
![Language](https://img.shields.io/badge/Language-Hindi%20Only-important)

BhashaSetu is an end-to-end conversational AI pipeline that operates exclusively in Hindi, combining Automatic Speech Recognition (ASR), Natural Language Processing (NLP) using LLMs, and Text-to-Speech (TTS) conversion.

## ğŸŒŸ Features

- **Hindi-Only Conversational Pipeline**  
- **Modular Architecture** (ASR, NLP, TTS handlers)
- Dual Interface:  
  - Terminal-based testing (`testing.py`)  
  - Web interface with browser-based ASR  
- Ollama-powered Llama3-3B model for NLP  
- gTTS for Hindi speech synthesis  

## ğŸ“¦ Installation

1. **Clone Repository**
```bash
git clone https://github.com/yourusername/BhashaSetu.git
cd BhashaSetu
```

2. **Create Virtual Environment**  
```bash
python -m venv venv
source venv/bin/activate  # Linux/MacOS
venv\Scripts\activate     # Windows
```

3. **Install Dependencies**
```bash
pip install -r requirements.txt
```

4. **Setup Ollama**  
Install [Ollama](https://ollama.ai/) and pull the model:
```bash
ollama pull llama3:3b
```

## ğŸš€ Usage

### Terminal Interface
```bash
python testing.py
```
- Speaks into microphone for input
- Processes through ASR â†’ NLP â†’ TTS
- Outputs audio response

### Web Interface
1. Start Flask Server:
```bash
python src/app.py
```

2. Open in Browser:
```
http://localhost:5000
```
- Chrome/Edge recommended
- Click microphone icon to speak
- Receive audio response

## ğŸ’‚ï¸ Project Structure

```
BhashaSetu/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py               # Flask API endpoints
â”‚   â”œâ”€â”€ asr/                 # Speech recognition
â”‚   â”œâ”€â”€ llm/                 # Language model handling
â”‚   â”œâ”€â”€ tts/                 # Text-to-speech
â”‚   â””â”€â”€ utils/               # Audio processing
â”œâ”€â”€ index.html               # Frontend UI
â”œâ”€â”€ script.js                # Browser interactions
â”œâ”€â”€ style.css                # UI styling
â”œâ”€â”€ testing.py               # CLI interface
â””â”€â”€ requirements.txt         # Dependencies
```

## ğŸ”§ Components

### ASR (Speech Recognition)
- Browser Web Speech API (frontend)
- Python speech recognition library (CLI)

### NLP (Llama3-3B)
- Local inference via Ollama
- Hindi language processing
- Conversation context management

### TTS (Text-to-Speech)
- gTTS Hindi synthesis
- Audio streaming for web interface
- MP3 file generation for CLI

## ğŸŒ API Endpoint

`POST /process_stream`
- Accepts JSON: `{"text": "à¤¹à¤¿à¤‚à¤¦à¥€ à¤Ÿà¥‡à¤•à¥à¤¸à¤Ÿ"}`
- Returns: 
  ```json
  {
    "audio_url": "/generated/audio.mp3",
    "response_text": "à¤¹à¤¿à¤‚à¤¦à¥€ à¤ªà¥à¤°à¤¤à¤¿à¤•à¥à¤°à¤¿à¤¯à¤¾"
  }
  ```

## ğŸ’¡ Notes

- **Browser Compatibility**  
  Requires Chrome/Edge for Web Speech API
- **Ollama Requirements**  
  Ensure Ollama service is running:
  ```bash
  ollama serve
  ```
- **Environment Variables**  
  Create `.env` for custom configurations:
  ```
  OLLAMA_ENDPOINT=http://localhost:11434
  TEMP_AUDIO_PATH=./generated
  ```

## ğŸ¤ Contributing

1. Fork repository
2. Create feature branch:
```bash
git checkout -b feature/your-feature
```
3. Commit changes:
```bash
git commit -m 'Add awesome feature'
```
4. Push to branch:
```bash
git push origin feature/your-feature
```
5. Open Pull Request

## ğŸ“„ License

Distributed under MIT License. See `LICENSE` for details.

---

Made with â¤ï¸ in India | Powered by Ollama & gTTS | [Contribute](#contributing)
